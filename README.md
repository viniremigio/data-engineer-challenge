# Data Engineer Challenge


Requirements
- Spark 2.4.5
- Python libs: pweave, Bokeh and boto3

```
spark-submit src/process.py file:///Users/vini/Downloads/data-sample_data-nyctaxi-trips-*-json_corrigido.json file:///Users/vini/Downloads/data-vendor_lookup-csv.csv file:///Users/vini/Downloads/data-payment_lookup-csv.csv test-vinilab data-engineer-challenge/output
```

Data pipeline is structured in following steps:

- **Collect**: data retrieved from Mixpanel API and then saved locally.
- **Process**: data wrangling process the data using **Spark**
- **Index**: The dataframe generated by data wrangling step is then indexed into **Elasticsearch**, so we can visualize better the data with Kibana
- **Data pipeline DAG**: It is a pipeline automation using **Airflow** that can be scheduled. This pipeline control the execution of the steps above

## Output files
The files are available at folder **data**. They are divided in 2 folders:
- collected: Contains JSON data collected from Mixpanel API, separate by day and event type
- processed: Contains CSV daily processed using Spark.

## Demo Video
Available as a private video in Youtube: **https://youtu.be/xDYy_XSGYHg**

## How to run

First, install required libs using *pip*:
```
pip install -r requirements.txt
```
The project was all written using **Python 3.7**. The steps could be running individually as follows:

### Collect
```
python src/collect.py yyyy-mm-dd SEXRET_KEY
```

### Process
```
spark-submit src/process.py yyyy-mm-dd
```

You also need to install Spark locally:
- Download [Spark 2.4.4](https://spark.apache.org/downloads.html) and install in folder */opt/spark*
- **Warning: Spark works only with jdk8!!!**

### Index
```
python src/index.py yyyy-mm-dd
```
If you want to install Elasticsearch and Kibana locally, you can use Docker images:
```
docker run -d -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" --name elasticsearch docker.elastic.co/elasticsearch/elasticsearch:7.5.0
docker run -d --link elasticsearch:elasticsearch -p 5601:5601 docker.elastic.co/kibana/kibana:7.5.0
```
Kibana will be available at *http://localhost:5601*

## Airflow

If you do not want to run the data pipeline step by step, you can run using Airflow:

Installing Airflow: 
```
pip install apache-airflow
```

Copy the *airflow.cfg* file from project to *${AIRFLOW_HOME}*

Then you need to run the following to initialize Airflow:
```
airflow initdb
airflow webserver -p 8080
airflow scheduler
```
Airflow will be available at *http://localhost:8080*

Finally, you can access Airflow interface to start the data pipeline execution.




